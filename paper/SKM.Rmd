---
title: "Sparse Kernels Methods (SKM), a general purpose machine learning R library"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
author:
  - "Osval Antonio Montesinos López$^1$"
  - "Abelardo Montesinos López$^2$"
  - "Brandon Alejandro Mosqueda González$^3$"
bibliography: references.bib
csl: /home/bmosqueda/Desktop/notes/cls/ieee.csl
output:
  bookdown::pdf_document2:
    toc: false
    number_sections: false
    includes:
      in_header: "/home/bmosqueda/dotfiles/header_rmd.tex"
  bookdown::word_document2:
    toc: false
    number_sections: false
    includes:
      in_header: "/home/bmosqueda/dotfiles/header_rmd.tex"
---

$^1$Facultad de Telemática, Universidad de Colima, Colima, México, $^2$Centro Universitario de Ciencias Exactas e Ingenierías (CUCEI), Universidad de Guadalajara, Guadalajara, México, $^3$Centro de Investigación en Computación (CIC), Instituto Politécnico Nacional (IPN), México city, México.

\Large

# Abstract

\large

In this paper we present a new R package for implementing 6 of the most popular machine learning algorithms with the (optional) use of sparse kernels.

\Large

# Introduction

Machine learning has become the main approch for solving problemas based on data. We can find applications that use machine learning to solve complex problems everywhere, from devices and digital services of daily use as smartphones and websites to scientific research of many areas of knowledge [@crosscheck; @genomic_selection; @speech_recognition; @object_detection]. As machine learning research has progressed, the supply and demand of software that facilitates its implmentation too, for this reason nowadays we can find numerous open source packages for data related tasks and machine learning algorithms [@tensorflow; @pandas; @dplyr; @h2o].

One of the most used programming languages for data analysis is R [@R], due to its statistical computing focus, free and open source software nature and the thousands of packages that extends its power to all kind of analysis and all related tasks of data science. It is difficult to find a machine learning algorithm which does not exist a package for implementing it in R, even it can be said that some of the R packages contains more complete/specialized implementations [@e1071; @randomForestSRC; @glmnet] than those available in other programming languages. As machine learning is strongly based in statistical models and as R is the _de facto_ language for statistics research, those who want to do machine learning sooner or later they will encounter R.

The majority of R's packages of machine learning algorithms includes one type of model or a family of similar models and each package was developed by different people, which, together the fact that in R does not exist a code style guideline or standard widely used make harder the use of the packages because in most cases to train a model you have to learn about the package that includes it, the expected data format, the name and expected parameters and the code convention (if any) used by the package. In addition to this, you may need other packages to perform cross validation of your models, hyperparameter tuning, compute accuracy metrics, etc. There are some libraries that seek to integrate a wide range of tools needed for machine learning in one place, for example, scikit-learn [@scikit-learn] in Python, H2O [@h2o] in Java (with both R and Python versions), caret [@caret], mlr3 [@mlr3] and tidymodels [@tidymodels] in R. All of these options have their own philosophy and they were designed using different approaches to do machine learning.

We consider the mlr3 package the most powerful R package for machine learning in terms of the things you can do with it. mlr3 is an object oriented solution for machine learning focused in extensibility since it does not implement any model itself, but provide a unified interface to many existing packages in R. This is its major advantage but it is clear that such approach does not solve completely the dependency of other package which requieres the user to learn a little of both the package that implements the model and mlr3. Learning how to use all the mlr3 environment it is worthwhile because it also has efficient implementation of most data related tasks, parallelization, hyperparameter tuning, feature selection, etc. But you have to get used to the way it works and how things are defined in parts with the object oriented paradigm, not so common in R programming.

On the other hand we have caret and tidymodels which both packages provide therir own uniform interface to use them, a very important factor in good quality software. In the same way as mlr3, these two packages use third party packages of machine learning algorithms under the hood to train models, even with different options for the same algorithm. Caret is the oldest of these 3 packages, for this reason it still enjoys of considerable popularity while the major advantage of tidymodels is that belongs to the tidyverse, a collection of R packages designed for data science that share an underlying design philosophy, grammar and data structures [@tidyverse], so if you are familiar with tidyverse packages, you are going to start using tidymodels in a natural way.

In this work we present SKM (Sparse Kernels Methods), a new R package for machine learning that includes functions for model training, tuning, prediction, metrics evaluation and sparse kernels computation. The main goal of this package is not to provide a full toolkit for data analysis as the previouos packages we have already mentioned nor to include a interface to so many other packages, but focus specifically in 6 supervised type of models. The model functions in SKM were designed keeping in mind simplicity, so the parameters, hyperparameters and tuning specifications are defined directly when calling the function and the user only have to see one example to understand how the package works. We strive to make clearer the documentation following a base convention in the functions and all the parameters are validated with checkmate [@checkmate] in order to inform the user when an error ocurrs with meaningul error messages, something that many other packages neglect. The most important hyperparameters of each model can be tuned with two different methods, grid search and bayesian optimization [@gaussian_processes] (based on the code of rBayesianOptimization [@rBayesianOptimization] package). Although bayesian optimization is very popular and effective method of tuning, the mlr3 and caret packages does not offer it as option.

Sparse kernels have proven to be useful in helping the conventional machine learning algorithms capture non-linear patterns in data [@sparse_kernels]. To the best of our knowledge, there is no R package for sparse kernels computation so this is one of biggest contributions of SKM, which is also what gives it its name.

As software developers and consumers we are aware of the importance of share our work with the community, besides SKM is based completely on open source software, so we release this package under the GNU Lesser General Public License v3.0 (LGPLv3), so anyone can explore the source code, make modifications and develop other tools from this.

# Machine learning algorithms

SKM package includes 6 different functions of supervised machine learning algorithms. Each of these functions is one abstraction layer more to other R package that implements a family of algorithms, these are: 1) generalized boosted machines, which internally uses gbm [@gbm] package, 2) generalized linear models of glmnet [@glmnet] package, support vector machines of e1071 [@e1071] package, random forest of randomForestSRC [@randomForestSRC] package, bayesian regression models of BGLR [@BGLR] package and feed forward artificial neural networks of keras [@keras] package. The additional layer of abstraction allows all functions to share the same data input format, internally data is adapted to the expected format of each package, the result and prediction objects returned by these functions are also in the same format. Other benefit of these functions is that parameters that can be inferred from data itself are not asked to the user but set automatically, for example, the _family_ parameter of glmnet package which has to be "gaussian" for continuous response variables, "binomial" for binary variables, "multinomial" for multicategorical variables and "poisson" for count variables, can be inferred from the response variable. In addition, the same functions allow to specify in a easy and user friendly way the hyperparameters and tunining details without the need to call another function or instantiate another object but as arguments of the same function. In theory, as it happens with all the packages that calls internally functions of other packages the expected improvement is with regard to ease of use and extended functionality with a little increment in computational demand for the extra operations required, but since these operations are computationally low cost, there is no significant loss of power.

We have included in [Appendix A](#appendix_a) some comparative examples of the equivalent implementation of some machine learning models with mlr3, SKM and with the original package.

# Sparse Kernels

As [@sparse_kernels] points out, kernel methods transform the independent variables (inputs) using a kernel function, followed by the application of conventional machine learning techniques to the transformed data to achieve better results. Kernel methods are excellent options when dealing with complex data that shows non-linear patterns for their computational efficiency and because it can be used with any type of predictive machine. So, we have included the _kernelize_ function in SKM that can compute the same 7 kernels and their sparse versions described in [@sparse_kernels]: Linear, Polynomial, Sigmoid, Gaussian, Exponential, Arc-Cosine 1 And Arc-Cosine L. The  kernel computation is independent from the model fitting process, which allows the use of _kernelilze_ function with other packages or the machine learning algorithms implementations of SKM without kernels.

TODO: ventajas de la reducción de la dimensionalidad en los kernels en general y principalmente en los sparse.

In [Appendix B](#appendix_b) we have included some examples about how you can use the _kernelilze_ function of SKM with several type of data and then fit a model with it.

# Evaluation metrics

Evaluating models' performance is an important task of all machine learning workflow, for this reason we have included in SKM functions of the most popular metrics to evaluate models performance for both regression and classification. The regression metrics included are: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Normalized Root Mean Squared Error (NRMSE, with 4 types of normalization), Mean Absolute Error (MAE) and Mean Arctangent Absolute Percentage Error (MAAPE). The classification metrics included are: accuracy, specificity, sensitivity, Kappa coefficient, Brier score, Matthews correlation coefficient, precision, recall, Area Under the ROC Curve (ROC-AUC), Precision-Recall Area Under the Curver (PR-AUC) and F1 score.
In addition to the functions already mentioned, the wrapper functions numeric_summary and categorical_summary computes all the regression and classification metrics, respectively, in order to have a complete summary of your model's performance in a simple function.

As expected, all these metrics functions work in harmony with the machine learning algorithms functions, since they use the same data format and no extra data processing is necessary when using together. This does not limit or complicate the use of them with other packages, on the contrary, detailed documentation is provided.

[Appendix A](#appendix_a) and [Appendix B](#appendix_b) includes examples of some metrics functions that basically receive the observed and predicted values (or probabilities in classification) and returns a numeric value.

# Installation

SKM is a package developed to be used in the R ecosystem. A stable version is avaible at The Comprehensive R Archive Network (CRAN), the main repository for R packages, so you can install SKM from an R terminal with the following command:

```{r cran_installation, eval = FALSE}
install.packages("SKM")
```

As an open source project, we have published the package in a Github repository at [https://github.com/brandon-mosqueda/SKM](#https://github.com/brandon-mosqueda/snake-game) where you can find the full source code and other way of install the development version (and most updated) of the package. This development version may include correction of reported bugs, new functionalities, etc. In this repository you can also find a place to report bugs or contribute to the project. In order to install the development version you have to execute the following commands in an R terminal.

```{r github_installation, eval = FALSE}
if (!require("devtools")) {
  install.packages("devtools")
}

devtools::install_github("brandon-mosqueda/SKM")
```

# Conclusions and future work

This new package will benefit both machine learning practitioners and researchers who want to implement predictive models in a simple way. We also expect that people from different areas who are not programming experts can take advantage of the simplicity of SKM to enter to the machine learning world.

It is of special interest the _kernelize_ function in SKM since this is the first package which allows the use of kernels with different machine learning algorithms as a new approach of working with complex non-linear and high dimensional data.

It is not intendend to provide a full data science solution with this new package but new machine learning algorithms can be included in future versions along more metrics functions, models benchmarking, data imputation and other data science related tools.

# References

<div id="refs"></div>

# Appendix A {#appendix_a}

In this section we present the comparative implementation of the same machine learning model using mlr3, SKM and the original package.

## Random forest for regression

We are going to implement a random forest model using the famous iris dataset. This dataset includes the numeric variables Sepal.Length, Sepal.Width, Petal.Length and Petal.Width, and the categorical variable Species. Our model will predict the Sepal.Length variable using as predictors the remaining ones.

**SKM version:**

```{r skm_random_forest, eval = FALSE}
library(SKM)

set.seed(1)

# Data preparation
x <- to_matrix(iris[, -1])
y <- iris$Sepal.Length

# Select 80% of data as training and 20% as testing
records_number <- nrow(x)
training_indices <- sample(records_number, records_number * 0.8)
testing_indices <- seq(records_number)[-training_indices]

# Divide data
x_training <- x[training_indices, ]
x_testing <- x[testing_indices, ]
y_training <- y[training_indices]
y_testing <- y[testing_indices]

# Fit a model using Bayesian optimization for hyperparameter
# tuning
model <- random_forest(
  x_training,
  y_training,
  trees_number = list(min = 50, max = 200),
  sampled_x_vars_number = list(min = 0.3, max = 0.8),
  node_size = list(min = 2, max = 20),
  tune_type = "bayesian_optimization",
  tune_bayes_samples_number = 5,
  tune_bayes_iterations_number = 10
)

# Predict the testing samples
predictions <- predict(model, x_testing)

# Compute the summary of observed vs predicted
summary <- numeric_summary(y_testing, predictions$predicted)
summary
#> * MSE: 0.1012
#> * RMSE: 0.3181
#> * NRMSE: 0.4725
#> * MAAPE: 0.0428
#> * MAE: 0.2338
#> * Pearson correlation: 0.8929
```

**ml3 version:**

```{r ml3_random_forest, eval = FALSE}
# If you do not have installed mlr3 packages install them with the
# following commands:
# install.packages("mlr3verse")
# devtools::install_github("mlr-org/mlr3extralearners")

library(mlr3verse)
library(mlr3extralearners)
library(mlr3tuning)

# Create the regression task with the iris dataset to predict the
# Sepal.Length variable.
task_iris <- as_task_regr(
  iris,
  target = "Sepal.Length",
  id = "iris"
)

# Create a randomForestSRC based learner.
learner_iris <- lrn("regr.rfsrc")

# Define the terminator criteria as 15 iterations in tuning
terminator <- trm("evals", n_evals = 15)

# Define the hyperparameters characteristics
search_space <- ps(
  ntree = p_int(lower = 50, upper = 200),
  mtry = p_int(lower = 1, upper = 4),
  nodesize = p_int(lower = 2, upper = 20)
)

# Resampling (cross validation) strategy
hout <- rsmp("holdout")
# Set the loss function to be used during tuning
measure <- msr("regr.mse")

# Initialize a tuner
tuner_instance <- TuningInstanceSingleCrit$new(
  task = task_iris,
  learner = learner_iris,
  resampling = hout,
  measure = measure,
  search_space = search_space,
  terminator = terminator
)

# Define the tuner with the tune strategy
tuner <- tnr("grid_search", resolution = 5)
tuner$optimize(tuner_instance)

# Set the best hyperparameters combination to the learner
learner_iris$param_set$values <-
  tuner_instance$result_learner_param_vals
learner_iris$train(task_iris, row_ids = training_indices)

predictions <- learner_iris$predict(
  task_iris,
  row_ids = testing_indices
)

numeric_summary(predictions$truth, predictions$response)
#> * MSE: 0.0993
#> * RMSE: 0.3151
#> * NRMSE: 0.4679
#> * MAAPE: 0.0433
#> * MAE: 0.2351
#> * Pearson correlation: 0.8899
```

**randomForestSRC (original package) version:**

```{r original_random_forest, eval = FALSE}
library(randomForestSRC)

data_training <- iris[training_indices, ]
data_testing <- iris[testing_indices, ]

model <- rfsrc(
  Sepal.Length ~ .,
  data_training,
  ntree = 200,
  mtry = 3,
  nodesize = 5
)
predictions <- predict(model, newdata = data_testing)

numeric_summary(data_testing$Sepal.Length, predictions$predicted)
#> * MSE: 0.0916
#> * RMSE: 0.3026
#> * NRMSE: 0.4494
#> * MAAPE: 0.0406
#> * MAE: 0.2205
#> * Pearson correlation: 0.8988
```

Note that all versions implements random forest using randomForestSRC package and the last implementation no tunining is performed.

# Appendix B {#appendix_b}
