---
title: "Sparse Kernels Methods (SKM), a general purpose machine learning R library"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
author:
  - "Osval Antonio Montesinos López$^1$"
  - "Brandon Alejandro Mosqueda González$^2$"
  - "Abelardo Montesinos López$^3$"
bibliography: references.bib
csl: /home/bmosqueda/Desktop/notes/cls/ieee.csl
output:
  bookdown::pdf_document2:
    toc: false
    number_sections: false
    includes:
      in_header: "/home/bmosqueda/dotfiles/header_rmd.tex"
  bookdown::word_document2:
    toc: false
    number_sections: false
    includes:
      in_header: "/home/bmosqueda/dotfiles/header_rmd.tex"
---

$^1$Facultad de Telemática, Universidad de Colima, Colima, Mexico, $^2$Centro de Investigación en Computación (CIC), Instituto Politécnico Nacional (IPN), Mexico city, Mexico, $^3$Centro Universitario de Ciencias Exactas e Ingenierías (CUCEI), Universidad de Guadalajara, Guadalajara, Mexico.

\Large

# Abstract

\large

The growing increase of data have allowed that machine learning methods are applied in many areas. The adoption of machine learning in other areas outside computer science have been facilitated thanks to the development of friendly software tools that whoever with a intermediate understanding of programming can use it. Some of these tools are complete libraries which offer solutions for different problems of data analysis but among so many utilities it can take quite some time to understand how to use it well. In this paper we present a new R package for implementing 6 of the most popular machine learning algorithms with the (optional) use of sparse kernels. This new package focuses in simplicity, it does not try to include all the available machine learning algorithms but the most important aspects of these 6 algorithms in an easy way. The other relevant contribution of this package is a function for the computation of 7 different kernels and their sparse versions which enables the creation of kernel machines without modifying the statistical machine learning algorithm, so its use is not restricted to this package only but can also be used with other packages without problems.

**Keywords:** R package, machine learning, sparse kernels.

\Large

# Introduction

Machine learning has become the main approach for solving problems based on data. We can find applications that use machine learning to solve complex problems everywhere, from devices and digital services of daily use as smartphones and websites to scientific research of many areas of knowledge [@crosscheck; @genomic_selection; @speech_recognition; @object_detection]. As machine learning research has progressed, the supply and demand of software that facilitates its implementation too, for this reason nowadays we can find numerous open source packages for data related tasks and machine learning algorithms [@tensorflow; @pandas; @dplyr; @h2o].

One of the most used programming languages for data analysis is R [@R], due to its statistical computing focus, free and open source software nature and the thousands of packages that extends its power to all kind of analysis and all related tasks of data science. It is difficult to find a machine learning algorithm which does not exist a package for implementing it in R, even it can be said that some of the R packages contains more complete/specialized implementations [@e1071; @randomForestSRC; @glmnet] than those available in other programming languages. As machine learning is strongly based in statistical models and as R is the _de facto_ language for statistics research, those who want to do machine learning sooner or later they will encounter R.

The majority of R's packages of machine learning algorithms includes one type of model or a family of similar models and each package was developed by different people, which, together with the fact that in R does not exist a code style guideline or standard widely used make harder the use of the packages because in most cases to train a model you have to learn about the package that includes it, the expected data format, the name and expected parameters and the code convention (if any) used by the package. In addition to this, you may need other packages to perform cross validation of your models, hyperparameter tuning, compute accuracy metrics, etc. There are some libraries that seek to integrate a wide range of tools needed for machine learning in one place, for example, scikit-learn [@scikit-learn] in Python, H2O [@h2o] in Java (with both R and Python versions), caret [@caret], mlr3 [@mlr3] and tidymodels [@tidymodels] in R. All of these options have their own philosophy and they were designed using different approaches to do machine learning.

We consider the mlr3 package the most powerful R package for machine learning in terms of the things you can do with it. mlr3 is an object oriented solution for machine learning focused in extensibility since it does not implement any model itself, but provide a unified interface to many existing packages in R. This is its major advantage but it is clear that such approach does not solve completely the dependency of other package which requires the user to learn a little of both the package that implements the model and mlr3. Learning how to use all the mlr3 environment it is worthwhile because it also has efficient implementation of most data related tasks, parallelization, hyperparameter tuning, feature selection, etc. But you have to get used to the way it works and how things are defined in parts with the object oriented paradigm, not so common in R programming.

On the other hand we have caret and tidymodels which both packages provide their own uniform interface to use them, a very important factor in good quality software. In the same way as mlr3, these two packages use third party packages of machine learning algorithms under the hood to train models, even with different options for the same algorithm. Caret is the oldest of these 3 packages, for this reason it still enjoys of considerable popularity while the major advantage of tidymodels is that belongs to the tidyverse, a collection of R packages designed for data science that share an underlying design philosophy, grammar and data structures [@tidyverse], so if you are familiar with tidyverse packages, you are going to start using tidymodels in a natural way.

In this work we present SKM (Sparse Kernels Methods), a new R package for machine learning that includes functions for model training, tuning, prediction, metrics evaluation and sparse kernels computation. The main goal of this package is not to provide a full toolkit for data analysis as the previous packages we have already mentioned nor to include a interface to so many other packages, but focus specifically in 6 supervised type of models, explained in the next section. The model functions in SKM were designed keeping in mind simplicity, so the parameters, hyperparameters and tuning specifications are defined directly when calling the function and the user only have to see one example to understand how the package works. We strive to make clearer the documentation following a base convention in the functions and all the parameters are validated with checkmate [@checkmate] in order to inform the user when an error occurrs with meaningful error messages, something that many other packages neglect. The most important hyperparameters of each model can be tuned with two different methods, grid search and Bayesian optimization [@gaussian_processes] (based on the code of rBayesianOptimization [@rBayesianOptimization] package). Although Bayesian optimization is very popular and effective method of tuning, the mlr3 and caret packages does not offer it as option.

Sparse kernels have proven to be useful in helping the conventional machine learning algorithms capture non-linear patterns in data [@sparse_kernels]. To the best of our knowledge, there is no R package for sparse kernels computation so this is one of biggest contributions of SKM, which is also what gives it its name.

As software developers and consumers we are aware of the importance of share our work with the community, besides SKM is based completely on open source software, so we release this package under the GNU Lesser General Public License v3.0 (LGPLv3), so anyone can explore the source code, make modifications and develop other tools from this.

# Machine learning algorithms

SKM package includes 6 different functions of supervised machine learning algorithms. Each of these functions is one abstraction layer more to other R package that implements a family of algorithms, these are: 1) generalized boosted machines, which internally uses gbm [@gbm] package, 2) generalized linear models of glmnet [@glmnet] package, 3) support vector machines of e1071 [@e1071] package, 4) random forest of randomForestSRC [@randomForestSRC] package, 5) bayesian regression models of BGLR [@BGLR] package and 6) deep neural networks of keras [@keras] package. The additional layer of abstraction allows all functions to share the same data input format, internally data is adapted to the expected format of each package, the result and prediction objects returned by these functions are also in the same format. Other benefit of these functions is that parameters that can be inferred from data itself are not asked to the user but set automatically, for example, the _family_ parameter of glmnet package which has to be "gaussian" for continuous response variables, "binomial" for binary variables, "multinomial" for multicategorical variables and "poisson" for count variables, can be inferred from the response variable. In addition, the same functions allow to specify in a easy and user friendly way the hyperparameters and tuning details without the need to call another function or instantiate another object but as arguments of the same function. In theory, as it happens with all the packages that calls internally functions of other packages the expected improvement is with regard to ease of use and extended functionality with a little increment in computational demand for the extra operations required, but since these operations are computationally low cost, there is no significant loss of power.

We have included in [Appendix A](#appendix_a) some comparative examples of the equivalent implementation of some machine learning models with mlr3, SKM and with the original package.

# Sparse Kernels

As [@sparse_kernels] points out, kernel methods transform the independent variables (inputs) using a kernel function, followed by the application of conventional machine learning techniques to the transformed data to achieve better results. Kernel methods are excellent options when dealing with complex data that shows non-linear patterns for their computational efficiency and because it can be used with any type of predictive machine. So, we have included the _kernelize_ function in SKM that can compute the same 7 kernels and their sparse versions described in [@sparse_kernels]: Linear, Polynomial, Sigmoid, Gaussian, Exponential, Arc-Cosine 1 And Arc-Cosine L. The  kernel computation is independent from the model fitting process, which allows the use of _kernelilze_ function with other packages or the machine learning algorithms implementations of SKM without kernels.

One of the major advantages of the use of sparse kernels is data dimensionality reduction since the number of independent variables (columns) is reduced at most the size as the number of records (rows) without losing much information. This is useful when working with high dimensional data where the number of columns is considerably greater than number of rows since there is less data and the training process of the model is more efficient.

In [Appendix B](#appendix_b) we have included some examples about how you can use the _kernelilze_ function of SKM with several type of data and then fit a model with it.

# Evaluation metrics

Evaluating models' performance is an important task of all machine learning workflow, for this reason we have included in SKM functions of the most popular metrics to evaluate models performance for both regression and classification problems. The regression metrics included are: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Normalized Root Mean Squared Error (NRMSE, with 4 types of normalization: by standard deviation, mean, range and interquantile range), Mean Absolute Error (MAE) and Mean Arctangent Absolute Percentage Error (MAAPE). The classification metrics included are: accuracy, specificity, sensitivity, Kappa coefficient, Brier score, Matthews correlation coefficient, precision, recall, Area Under the ROC Curve (ROC-AUC), Precision-Recall Area Under the Curver (PR-AUC), F1 score and a function to compute the confusion matrix.
In addition to the functions already mentioned, the wrapper functions numeric_summary and categorical_summary computes all the regression and classification metrics, respectively, in order to have a complete summary of your model's performance in a simple function.

As expected, all these metrics functions work in harmony with the machine learning algorithms functions, since they use the same data format and no extra data processing is necessary when using together. This does not limit or complicate the use of them with other packages, on the contrary, detailed documentation is provided.

[Appendix A](#appendix_a) and [Appendix B](#appendix_b) includes examples of some metrics functions that basically receive the observed and predicted values (or probabilities in classification) and returns a numeric value.

# Installation

SKM is a package developed to be used in the R ecosystem. A stable version is available at The Comprehensive R Archive Network (CRAN), the main repository for R packages, so you can install SKM from an R terminal with the following command:

```{r cran_installation, eval = FALSE}
install.packages("SKM")
```

As an open source project, we have published the package in a GitHub repository at [https://github.com/brandon-mosqueda/SKM](#https://github.com/brandon-mosqueda/snake-game) where you can find the full source code and another way of install the development version (and most updated) of the package. This development version may include correction of reported bugs, new functionalities, etc. In this repository you can also find a place to report bugs or contribute to the project. In order to install the development version you have to execute the following commands in an R terminal.

```{r github_installation, eval = FALSE}
if (!require("devtools")) {
  install.packages("devtools")
}

devtools::install_github("brandon-mosqueda/SKM")
```

# Ilustrative examples

Next we will illustrate the use of the SKM library with two data sets very popular in genomic selection.

**Wheat data.** This data set was first used by Crossa et al. (2010) and also by Cuevas et al. (2016, 2017, 2019) and comprises 599 wheat lines from the CIMMYT Global Wheat Program evaluated in four international environments representing four basic agroclimatic regions (mega-environments). The phenotypic trait considered here was grain yield (GY) of the 599 wheat lines evaluated in each of the four mega-environments. The 599 wheat lines were genotyped using 1447 Diversity Array Technology (DArT) markers generated by Triticarte Pty. Ltd.

**Maize data.** This maize data set was included in Souza et al. (2017), comes from USP (Universidad Sao Paulo) and consists of 722 (with 722=2888 observaciones) maize hybrids obtained by crossing 49 inbred lines. The hybrids were evaluated in four environments (E1-E4) in Piracicaba and Anhumas, São Paulo, Brazil, in 2016. The hybrids were evaluated using an augmented block design, with two commercial hybrids as checks to correct for micro-environmental variation. At each site, two levels of nitrogen (N) fertilization were used. The experiment conducted under ideal N conditions received 100 kg ha-1 of N (30 kg ha-1 at sowing and 70 kg ha-1 in a coverage application) at the V8 plant stage, while the experiment with low N received 30 kg/ha of N at sowing. The parent lines were genotyped with an Affymetrix Axiom Maize Genotyping Array of 616 K SNPs. Markers with Minor Allele Frequency (MAF) of # 0.05 were removed. After applying QC, 54,113 SNPs were available to make the predictions.

# Discussion



# Conclusions and future work

This new package will benefit both machine learning practitioners and researchers who want to implement predictive models in a simple way with state-of-the art methods for tuning hyperparameters like Bayesian optimization. We also expect that people from different areas who are not programming experts can take advantage of the simplicity of SKM to enter to the machine learning world.

It is of special interest the _kernelize_ function in SKM since this is the first package which allows the use of kernels with different machine learning algorithms as a new approach of working with complex non-linear and high dimensional data.

It is not intended to provide a full data science solution with this new package but new machine learning algorithms can be included in future versions along more metrics functions, models benchmarking, data imputation and other data science related tools.

# References

<div id="refs"></div>

# Appendix A {#appendix_a}

In this section we present the comparative implementation of the same machine learning model using mlr3, SKM and randomForestSRC, the original package. With this example the benefits of SKM become evident because of its simplicity and code clarity.

We are going to implement a random forest model using the famous iris dataset. This dataset includes the numeric variables Sepal.Length, Sepal.Width, Petal.Length and Petal.Width, and the categorical variable Species. Our model will predict the Sepal.Length variable using as predictors the remaining ones.

## A1. SKM implementation

```{r skm_random_forest, eval = FALSE}
library(SKM)

set.seed(1)

# Data preparation
x <- to_matrix(iris[, -1])
y <- iris$Sepal.Length

# Create a fold with 80% of data as training and
# 20% as testing
fold <- cv_random(
  records_number = nrow(x),
  folds_number = 1,
  testing_proportion = 0.2
)
fold <- fold[[1]]

# Divide data
x_training <- x[fold$training, ]
x_testing <- x[fold$testing, ]
y_training <- y[fold$training]
y_testing <- y[fold$testing]

# Fit a model using Bayesian optimization for hyperparameter
# tuning
model <- random_forest(
  x_training,
  y_training,
  trees_number = list(min = 50, max = 200),
  sampled_x_vars_number = list(min = 0.3, max = 0.8),
  node_size = list(min = 2, max = 20),
  tune_type = "bayesian_optimization",
  tune_bayes_samples_number = 5,
  tune_bayes_iterations_number = 10
)

# Predict the testing samples
predictions <- predict(model, x_testing)

# Compute the summary of observed vs predicted
summary <- numeric_summary(y_testing, predictions$predicted)
summary
#> * MSE: 0.1099
#> * RMSE: 0.3315
#> * NRMSE: 0.3772
#> * MAAPE: 0.0488
#> * MAE: 0.273
#> * Pearson correlation: 0.9253
```

## A2. ml3 implementation

```{r ml3_random_forest, eval = FALSE}
# If you do not have installed mlr3 packages install them with
# the following commands:
# install.packages("mlr3verse")
# devtools::install_github("mlr-org/mlr3extralearners")

library(mlr3verse)
library(mlr3extralearners)
library(mlr3tuning)

# Create the regression task with the iris dataset to predict
# the Sepal.Length variable.
task_iris <- as_task_regr(
  iris,
  target = "Sepal.Length",
  id = "iris"
)

# Create a randomForestSRC based learner.
learner_iris <- lrn("regr.rfsrc")

# Define the terminator criteria as 15 iterations in tuning
terminator <- trm("evals", n_evals = 15)

# Define the hyperparameters characteristics
search_space <- ps(
  ntree = p_int(lower = 50, upper = 200),
  mtry = p_int(lower = 1, upper = 4),
  nodesize = p_int(lower = 2, upper = 20)
)

# Resampling (cross validation) strategy
hout <- rsmp("holdout")
# Set the loss function to be used during tuning
measure <- msr("regr.mse")

# Initialize a tuner
tuner_instance <- TuningInstanceSingleCrit$new(
  task = task_iris,
  learner = learner_iris,
  resampling = hout,
  measure = measure,
  search_space = search_space,
  terminator = terminator
)

# Define the tuner with the tune strategy
tuner <- tnr("grid_search", resolution = 5)
tuner$optimize(tuner_instance)

# Set the best hyperparameters combination to the learner
learner_iris$param_set$values <-
  tuner_instance$result_learner_param_vals
learner_iris$train(task_iris, row_ids = fold$training)

predictions <- learner_iris$predict(
  task_iris,
  row_ids = fold$testing
)

numeric_summary(predictions$truth, predictions$response)
#> * MSE: 0.1098
#> * RMSE: 0.3314
#> * NRMSE: 0.3771
#> * MAAPE: 0.0484
#> * MAE: 0.2704
#> * Pearson correlation: 0.9254
```

## A3. randomForestSRC (original package) implementation

```{r original_random_forest, eval = FALSE}
library(randomForestSRC)

data_training <- iris[fold$training, ]
data_testing <- iris[fold$testing, ]

model <- rfsrc(
  Sepal.Length ~ .,
  data_training,
  ntree = 200,
  mtry = 3,
  nodesize = 5
)
predictions <- predict(model, newdata = data_testing)

numeric_summary(
  data_testing$Sepal.Length,
  predictions$predicted
)
#> * MSE: 0.1094
#> * RMSE: 0.3307
#> * NRMSE: 0.3763
#> * MAAPE: 0.0492
#> * MAE: 0.2754
#> * Pearson correlation: 0.9256
```

Note that all versions implements random forest using under the hood the randomForestSRC package and in the last implementation no tuning is performed.

# Appendix B {#appendix_b}

In this section we present some examples of how to use the kernelize function in order to compute different kernels and their sparse versions. As the kernelize function receives a *data.frame* or a *matrix* and returns this data after applying the kernel as a matrix, such data can be used in conjunction with the models functions or any other package.

The following examples are going to use the iris dataset too for simplicity but you have to keep in mind that kernels are recommended to be used with high dimensional data.

```{r kernelize, eval = FALSE}
library(SKM)

set.seed(2)

y <- iris$Species
x_polynomial <- kernelize(
  iris[, -5],
  kernel = "polynomial",
  degree = 3
)
# After kernelize, x is now a square matrix
dim(x_polynomial)
#> [1] 150 150

x_sparse_polynomial <- kernelize(
  iris[, -5],
  kernel = "sparse_polynomial",
  rows_proportion = 0.6,
  degree = 4
)
# After the sparse kernel, x has at most as many columns as
# 60% rows
dim(x_sparse_polynomial)
#> [1] 150  60

# Now we use these new matrices to fit models
model <- generalized_linear_model(
  x_polynomial,
  y,
  alpha = 0
)

# We are going to predict the same samples used
# in training
predictions <- predict(model, x_polynomial)

confusion_matrix(y, predictions$predicted)
#             predicted
# observed     setosa versicolor virginica
#   setosa         50          0         0
#   versicolor      0         41         9
#   virginica       0          5        45

# Now we are going to fit a deep learning model
# using the sparse kernel matrix
sparse_model <- deep_learning(
  x_sparse_polynomial,
  y,
  epochs_number = 100,
  learning_rate = c(0.001, 0.01),
  layers = list(
    # First hidden layer
    list(
      neurons_number = c(50, 100),
      activation = "relu",
      dropout = 0.2
    ),
    # Second hidden layer
    list(
      neurons_number = c(50, 100),
      activation = "relu",
      dropout = 0.2
    )
  )
)

predictions <- predict(sparse_model, x_sparse_polynomial)

categorical_summary(
  observed = y,
  predicted = predictions$predicted,
  probabilities = predictions$probabilities
)

```

Even though in this example we only use the polynomial kernel and its sparse version, the same applies to all other available kernels, only it is necessary to specify the desired kernel.

# Appendix C {#appendix_c}

In this section we provide the code for implementing the 6 models: 1) generalized boosted machines 2) generalized linear models 3) support vector machines 4) random forest 5) Bayesian regression models and 6) deep neural networks for the wheat data set using Bayesian optimization. It is important to point out that in the predictor we included the information of Environments, Genotypes and Genotype$\times$Environment interaction.

# Appendix D {#appendix_d}

In this section we provide the code for implementing the 6 models: 1) generalized boosted machines 2) generalized linear models 3) support vector machines 4) random forest 5) Bayesian regression models and 6) deep neural networks for the maize data set using grid search. It is important to point out that in the predictor we included the information of Environments, Genotypes and Genotype$\times$Environment interaction.
