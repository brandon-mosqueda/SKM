% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deep_learning.R
\name{deep_learning}
\alias{deep_learning}
\title{Fit a Deep Learning Model}
\usage{
deep_learning(
  x,
  y,
  learning_rate = 0.001,
  epochs_number = 500,
  batch_size = 32,
  layers = list(list(neurons_number = 50, neurons_proportion = NULL, activation =
    "relu", dropout = 0, ridge_penalty = 0, lasso_penalty = 0)),
  output_penalties = list(ridge_penalty = 0, lasso_penalty = 0),
  tune_type = "Grid_search",
  tune_cv_type = "K_fold",
  tune_folds_number = 5,
  tune_testing_proportion = 0.2,
  tune_folds = NULL,
  tune_grid_proportion = 1,
  tune_bayes_samples_number = 10,
  tune_bayes_iterations_number = 10,
  optimizer = "adam",
  loss_function = NULL,
  with_platt_scaling = FALSE,
  platt_proportion = 0.3,
  shuffle = TRUE,
  early_stop = FALSE,
  early_stop_patience = 50,
  validate_params = TRUE,
  seed = NULL,
  verbose = TRUE
)
}
\arguments{
\item{x}{(\code{matrix}) The predictor (independet) variable(s). It must be a
numeric matrix. You can use \code{\link[=to_matrix]{to_matrix()}} function to convert your data to
to a \code{matrix}.}

\item{y}{(\code{data.frame} | \code{vector} | \code{matrix}) The response (dependent)
variable(s). If it is a \code{data.frame} or a \code{matrix} with 2 or more columns,
a multivariate model is assumed, a univariate model otherwise. In
univariate models if \code{y} is \code{character}, \code{logical} or \code{factor} a
categorical response is assumed. When the response is categorical with only
two classes a binary response is assumed, with more than two classes a
categorical response. When the response variable is numeric with only
integers values greater or equals than zero a count response is
assumed, a continuous response otherwise. In multivariate models response
variables can be of different types for mixed models.}

\item{learning_rate}{(\code{numeric}) (\strong{tunable}) This hyperparameter controls
how much to change the model in response to the estimated error each time
the model weights are updated. 0.001 by default.}

\item{epochs_number}{(\code{numeric}) (\strong{tunable}) An arbitrary cutoff, generally
defined as "one pass over the entire dataset", used to separate training
into distinct phases, which is useful for logging and periodic evaluation.
500 by default.}

\item{batch_size}{(\code{numeric}) (\strong{tunable}) A hyperparameter of gradient
descent that controls the number of training samples to work through before
the model's internal parameters are updated. 32 by default.}

\item{layers}{(\code{list}) The hidden layers. It must be a \code{list} of \code{list}'s
where each entry will represent a hidden layer in the neural network. Each
inner \code{list} can contain the following fields with a vector of values:
\itemize{
\item \code{"neurons_number"}: (\code{numeric}) (\strong{tunable}) The number of neurons in that
layer. 50 by default.
\item \code{"neurons_proportion"}: (\code{numeric}) (\strong{tunable}) Similar to
\code{"neurons_number"} but the provided values will be the proportion specified
times the number of columns in \code{x}, so a value of 1 means "use as many
neurons as columns in \code{x}", 0.5 means use as neurons number the half of
number of columns in \code{x}. This is combined with the values of
\code{"neurons_number"} for tuning. \code{NULL} by default.
\item \code{"activation"}: (\code{character}) (\strong{tunable}) (case not sensitive) The name
of the activation function to apply in this layer. The available activation
functions are \code{"linear"}, \code{"relu"}, \code{"elu"}, \code{"selu"}, \code{"hard_sigmoid"},
\code{"sigmoid"}, \code{"softmax"}, \code{"softplus"}, \code{"softsign"}, \code{"tanh"},
\code{"exponential"}. \code{"relu"} by default.
\item \code{"dropout"}: (\code{numeric}) (\strong{tunable}) The proportion of neurons randomly
selected and set to 0 at each step during training process, which helps
prevent overfitting. 0 by default.
\item \code{"lasso_penalty"}: (\code{numeric}) (\strong{tunable}) The regularization value <= 0
and <=1 for penalizing that layer with Lasso (a.k.a L1) penalty. 0 by
default (no penalty).
\item \code{"ridge_penalty"}: (\code{numeric}) (\strong{tunable}) The regularization value <= 0
and <= 1 for penalizing that layer with Ridge (a.k.a L2) penalty. Note that
if both penalization params (Ridge and Lasso) are sent, the ElasticNet
penalization is implemented, that is a combination of both of them. 0 by
default (no penalty).
}

You can provide as many \code{list}'s as you want, each of them representing a
hidden layer and you do not need to provide all the parameters. If one
parameter is not provided, the default value described above is used. By
default the next \code{list} is used:

\code{list( list( neurons_number = 50, neurons_proportion = NULL, activation = "relu", dropout = 0, ridge_penalty = 0, lasso_penalty = 0 ) )}}

\item{output_penalties}{(\code{list}) The penalty values for the output layer. The
list can contain the following two fields:
\itemize{
\item \code{"lasso_penalty"}: (\code{numeric}) (\strong{tunable}) The regularization value <= 0
and <=1 for penalizing that layer with Lasso (a.k.a L1) penalty. 0 by
default (no penalty).
\item \code{"ridge_penalty"}: (\code{numeric}) (\strong{tunable}) The regularization value <= 0
and <= 1 for penalizing that layer with Ridge (a.k.a L2) penalty. Note that
if both penalization params (Ridge and Lasso) are sent, the ElasticNet
penalization, is implemented that is a combination of both of them. 0 by
default (no penalty).
}

You do not have to provide the two values, if one of them is not provided the
default value is used. By default the next \code{list} is used:

\code{list( ridge_penalty = 0, lasso_penalty = 0 )}}

\item{tune_type}{(\code{character(1)}) (case not sensitive) The type of tuning to
perform. The options are \code{"Grid_search"} and "\code{Bayesian_optimization}".
\code{"Grid_search"} by default.}

\item{tune_cv_type}{(\code{character(1)}) (case not sensitive) The type of cross
validation to tune the model. The options are \code{"K_fold"} and
\code{"Random"}. \code{"K_fold"} by defaul.}

\item{tune_folds_number}{(\code{numeric(1)}) The number of folds to tune each
hyperparameter combination (k in k-fold cross validation). 5 by default.}

\item{tune_testing_proportion}{(\code{numeric(1)}) A number between (0, 1) to
specify the proportion of records to use as validation set when
\code{tune_cv_type} is \code{"Random"}. 0.2 by default.}

\item{tune_folds}{(\code{list}) Custom folds for tuning. It must be a \code{list} of
\code{list}'s where each entry will represent a fold. Each inner \code{list} has to
contain the fields \code{"training"} and \code{"testing"} with numeric vectors of
indices of those entries to be used as training and testing in each fold.
Note that when this parameter is set, \code{tune_cv_type}, \code{tune_folds_number}
and \code{tune_testing_proportion} are ignored. \code{NULL} by default.}

\item{tune_grid_proportion}{(\code{numeric(1)}) Only when \code{tune_type} is
\code{"Grid_search"}, a number between (0, 1] to specify the proportion of
hyperparameters combinations to sample from the grid and evaluate in
tuning (useful when the grid is big). 1 by default (full grid).}

\item{tune_bayes_samples_number}{(\code{numeric(1)}) Only when \code{tune_type} is
\code{"Bayesian_optimization"}, the number of initial random hyperparameters
combinations to evalute before the Bayesian optimization process. 10 by
default.}

\item{tune_bayes_iterations_number}{(\code{numeric(1)}) Only when \code{tune_type} is
\code{"Bayesian_optimization"}, the number of optimization iterations to
evaluate after the initial random samples specified in
\code{tune_bayes_samples_number}. 10 by default.}

\item{optimizer}{(\code{character(1)}) (case not sensitive) Algorithm used to
reduce the loss function and update the weights in backpropagation. The
available options are \code{"adadelta"}, \code{"adagrad"}, \code{"adamax"}, \code{"adam"},
\code{"nadam"}, \code{"rmsprop"} and \code{"sgd"}. \code{"adam"} by default.}

\item{loss_function}{(\code{character(1)}) (case not sensitive) The name of the
loss function the model will seek to minimize during training and tuning.
You can find the complete list of available loss functions in the Details
section below. This parameter can be used only in univariate analysis.
\code{NULL} by default which selects one automatically based on the type of the
response variable \code{y}: \code{"mean_squared_error"} for continuous, \code{"poisson"}
for counts, \code{"binary_crossentropy"} for binary and
\code{"categorical_crossentropy"} for categorical.}

\item{with_platt_scaling}{(\code{logical(1)}) Should Platt scaling be used to fit
the model and adjust the predictions? Only available for univariate models
with a numeric or binary response variable. For more information, see
Details section below. \code{FALSE} by default.}

\item{platt_proportion}{(\code{Ç¹umeric(1)}) The proportion of individuals used to
fit the linear model required for Platt scaling. Note that this parameter
is used only when \code{with_platt_scaling} is \code{TRUE}. 0.3 by default.}

\item{shuffle}{(\code{logical(1)}) Should the training data be shuffled before
each epoch? \code{TRUE} by default.}

\item{early_stop}{(\code{logical(1)}) Should the model stop training when the loss
function has stopped improving? \code{FALSE} by default.}

\item{early_stop_patience}{(\code{numeric(1)}) The number of epochs with no
improvement after which training will be stopped. Note that this parameter
is used only when \code{early_stop} is \code{TRUE}. 50 by default.}

\item{validate_params}{(\code{logical(1)}) Should the parameters be validated? It
is not recommended to set this parameter to \code{FALSE} because if something
fails a non meaningful error is going to be thrown. \code{TRUE} by default.}

\item{seed}{(\code{numeric(1)}) A value to be used as internal seed for
reproducible results. \code{NULL} by default.}

\item{verbose}{(\code{logical(1)}) Should the progress information be printed?
\code{TRUE} by default.}

\item{tune_loss_function}{(\code{character(1)}) (case not sensitive) The loss
function to use in tuning. The options are \code{"mse"}, \code{"maape"}, \code{"mae"},
\code{"nrmse"} or \code{"rmse"} when \code{y} is a numeric response variable,
\code{"accuracy"} or \code{"kappa_coeff"} when \code{y} is a categorical response
variable (including binary) and \code{"f1_score"}, \code{"roc_auc"} or \code{"pr_auc"}
when \code{y} is a binary response variable. \code{NULL} by default which uses
\code{"mse"} for numeric variables and \code{"accuracy"} for categorical variables.}
}
\value{
An object of class \code{"DeepLearningModel"} that inherits from classes
\code{"Model"} and \code{"R6"} with the fields:
\itemize{
\item \code{fitted_model}: An object of class \code{\link[keras:keras_model_sequential]{keras::keras_model_sequential()}} with the model.
\item \code{x}: The final \verb{\code{matrix}} used to fit the model.
\item \code{y}: The final \code{vector} or \code{matrix} used to fit the model.
\item \code{hyperparams_grid}: A \code{data.frame} with all the computed combinations of
hyperparameters and with one more column called \code{"loss"} with the value of
the loss function for each combination. The data is ordered with the best
combinations at start, sometimes with the lowest values first and other
times with the greatest values first, depending the loss function.
\item \code{best_hyperparams}: A \code{list} with the combination of hyperparameters with
the best loss value (the first row in \code{hyperparams_grid}).
\item \code{execution_time}: A \code{difftime} object with the total time taken to tune and
fit the model.
\item \code{removed_rows}: A \code{numeric} vector with the records' indices (in the
provided position) that were deleted and not taken in account in tunning
nor training.
\item \code{removed_x_cols}: A \code{numeric} vector with the columns' indices (in the
provided positions) that were deleted and not taken in account in tunning
nor training.
\item \code{...}: Some other parameters for internal use.
}
}
\description{
\code{deep_learning()} is a wrapper of the \code{\link[keras:keras_model_sequential]{keras::keras_model_sequential()}}
function with the ability to tune the hyperparameters (grid search) in a
simple way. It fits univariate and multivariate models for numeric and/or
categorical response variables.

All the parameters marked as (\strong{tunable}) accept a vector of values with
wich the grid is generated for grid search tuning or a list with the min
and max values for bayesian optimization tuning. The returned object contains
a \code{data.frame} with the hyperparameters combinations evaluated. In the end
the best combination of hyperparameters is used to fit the final model, which
is also returned and can be used to make new predictions.
}
\details{
You have to consider that before tuning all columns without variance
(where all the records has the same value) are removed. Such columns
positions are returned in the \code{removed_x_cols} field of the returned object.

All records with missing values (\code{NA}), either in \code{x} or in \code{y} will be
removed. The positions of the removed records are returned in the
\code{removed_rows} field of the returned object.

\subsection{Tuning}{

The general tuning algorithm works as follows:

\figure{tuning_algorithm.png}{Tuning algorithm}

For grid search tuning, the hyperparameters grid is generated (step one in
the algorithm) with the cartesian product of all the provided values (all the
posible combinations) in all \strong{tunable} parameters. If only one value of
each \strong{tunable} parameter is provided no tuning is done.
\code{tune_grid_proportion} allows you to specify the proportion of all
combinations you want to sample from the full grid and tune them, by default
all combinations are evaluated.

For bayesian optimization tuning, step one in the algorithm works a little
different. At start, \code{tune_bayes_samples_number} different
hyperparameters combinations are generated and evaluated, then
\code{tune_bayes_iterations_number} new hyperparameters combinations are generated
and evaluated iteratively based on the bayesian optimization algorithm, but
this process is equivalent to that described in the general tuninig
algorithm. Note that only the hyperparameters for which the list of min and
max values were provided are tuned and their values fall in the specified
boundaries.
}

\strong{Important:} Unlike the other models, when tuning deep learning models
steps 6 and 7 are omit in the algorithm, instead \code{train} and \code{test} datasets
are sent to \code{keras}, the first one to fit the model and the second one to
compute the loss function at the end of each epoch, so at the end, the saved
value in step 8 is the validation loss value returned by \code{keras} in the last
epoch.
\subsection{Last (output) layer}{

By default this function selects the activation function and the number of
neurons for the last layer of the model based on the response variable(s)
type(s). For continuous responses the \code{"linear"} (identity) activation
function is used with one neuron, for count responses the \code{"exponential"}
with one neuron, for binary responses the \code{"sigmoid"} with one neuron and for
categorical responses \code{"softmax"} with as many neurons as number of
categories.
}

\subsection{Platt scaling}{

TODO
}
}
\examples{
\dontrun{
# Fit with all default parameters
model <- deep_learning(to_matrix(iris[, -5]), iris$Species)

# With tuning
model <- deep_learning(
  to_matrix(iris[, -1]),
  iris$Sepal.Length,
  epochs_number = 10,
  learning_rate = c(0.1, 0.05),
  layers = list(list(neurons_number = 10, activation = "relu"))
)

predictions <- predict(model, to_matrix(iris[, -1]))
predictions$predicted

# See the whole grid
model$hyperparams_grid

# Multivariate analysis
model <- deep_learning(
  x = to_matrix(iris[, -c(1, 5)]),
  y = iris[, c(1, 5)],
  epochs_number = 10,
  layers = list(
    list(neurons_number = 10, activation = "relu"),
    list(neurons_number = 5, activation = "sigmoid")
  )
)
}

}
\seealso{
\code{\link[=predict.Model]{predict.Model()}}

Other models: 
\code{\link{bayesian_model}()},
\code{\link{generalized_boosted_machine}()},
\code{\link{generalized_linear_model}()},
\code{\link{partial_least_squares}()},
\code{\link{random_forest}()},
\code{\link{support_vector_machine}()}
}
\concept{models}
