% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/random_forest.R
\name{random_forest}
\alias{random_forest}
\title{Fit a Random Forest Model}
\usage{
random_forest(
  x,
  y,
  trees_number = 500,
  node_size = 5,
  node_depth = NULL,
  sampled_x_vars_number = NULL,
  tune_type = "Grid_search",
  tune_cv_type = "K_fold",
  tune_folds_number = 5,
  tune_testing_proportion = 0.2,
  tune_folds = NULL,
  tune_loss_function = NULL,
  tune_grid_proportion = 1,
  tune_bayes_samples_number = 10,
  tune_bayes_iterations_number = 10,
  split_rule = NULL,
  splits_number = 10,
  x_vars_weights = NULL,
  records_weights = NULL,
  na_action = "omit",
  validate_params = TRUE,
  seed = NULL,
  verbose = TRUE
)
}
\arguments{
\item{x}{(\code{matrix}) The predictor (independet) variable(s). It must be a
numeric matrix. You can use \code{\link[=to_matrix]{to_matrix()}} function to convert your data to
a design \code{matrix}.}

\item{y}{(\code{data.frame} | \code{vector} | \code{matrix}) The response (dependent)
variable(s). If it is a \code{data.frame} or a \code{matrix} with 2 or more columns,
a multivariate model is assumed, a univariate model otherwise. If \code{y} is
(or contains some columns) \code{character}, \code{logical} or \code{factor}, a
categorical variable is assumed, numeric otherwise. In multivariate models
categorical and numeric variables can be combined for a mixed model.}

\item{trees_number}{(\code{numeric}) (\strong{tunable}) Number of trees. 500 by
default.}

\item{node_size}{(\code{numeric}) (\strong{tunable}) Minimum size of terminal
nodes. 5 by default.}

\item{node_depth}{(\code{numeric}) (\strong{tunable}) Maximum depth to which a tree
should be grown. \code{NULL} (ignored) by default.}

\item{sampled_x_vars_number}{(\code{numeric}) (\strong{tunable}) Also known as \code{mtry}.
Number of variables randomly selected as candidates for splitting a node.
You can specify values > 0 and <= 1 with the proportion of variables in \code{x}
or directly the number of variables to use or a combination of both. \code{NULL}
by default (which uses \code{p / 3} with numeric response variables or \code{sqrt(p)}
otherwise, where \code{p} is the number of variables in \code{x}).}

\item{tune_cv_type}{(\code{character(1)}) (case not sensitive) The type of cross
validation to tune the model. The options are \code{"K_fold"} and
\code{"Random"}. \code{"K_fold"} by defaul.}

\item{tune_folds_number}{(\code{numeric(1)}) The number of folds to tune each
combination in the grid (k in k-fold cross validation). 5 by default.}

\item{tune_testing_proportion}{(\code{numeric(1)}) A number > 0 and < 1 to
specify the proportion of records to use as validation set when
\code{tune_cv_type} is \code{"Random"}. 0.2 by default.}

\item{tune_folds}{(\code{list}) Custom folds for tuning. It must be a \code{list} of
\code{list}'s where each entry will represent a fold. Each inner \code{list} has to
contain the fields \code{"training"} and \code{"testing"} with numeric vectors of
indices of those entries to be used as training and testing in each fold.
Note that when this parameter is set \code{tune_cv_type}, \code{tune_folds_number}
and \code{tune_testing_proportion} are ignored. \code{NULL} by default.}

\item{tune_loss_function}{(\code{character(1)}) (case not sensitive) The loss
function to use in tuning. The options are \code{"mse"}, \code{"maape"}, \code{"mae"},
\code{"nrmse"} or \code{"rmse"} when \code{y} is a numeric response variable,
\code{"accuracy"} or \code{"kappa_coeff"} when \code{y} is a categorical response
variable (including binary) and \code{"f1_score"}, \code{"roc_auc"} or \code{"pr_auc"}
when \code{y} is a binary response variable. \code{NULL} by default which uses
\code{"mse"} for numeric variables and \code{"accuracy"} for categorical variables.}

\item{tune_grid_proportion}{(\code{numeric(1)}) A number > 0 and <= 1 to specify
the proportion of combinations to sample from the grid and evaluate in
tuning (useful when the grid is big). 1 by default.}

\item{split_rule}{(\code{character(1)}) (case not sensitive) Splitting rule. The
available options are \code{"mse"}, \code{"gini"}, \code{"auc"}, \code{"entropy"}. \code{NULL} by
default (which selects the best depending on the type of response
variable. For more information, see Details section below).}

\item{splits_number}{(\code{numeric(1)}) Non-negative integer value for number of
random splits to consider for each candidate splitting variable. 10 by
default.}

\item{x_vars_weights}{(\code{numeric}) Vector of non-negative weights (does not
have to sum to 1) representing the probability of selecting a variable for
splitting. \code{NULL} by default (uniform weights).}

\item{records_weights}{(\code{numeric}) Vector of non-negative weights (does not
have to sum to 1) for sampling cases. Observations with larger weights will
be selected with higher probability in the bootstrap (or subsampled)
samples. \code{NULL} by default (uniform weights).}

\item{na_action}{(\code{character(1)}) (case not sensitive) Action taken if the
data contains \code{NA}'s. The available options are \code{"omit"} (remove all
records with \code{NA}'s) and \code{"impute"} (impute missing values). \code{"omit"} by
default.}

\item{validate_params}{(\code{logical(1)}) Should the parameters be validated? It
is not recommended to set this parameter to \code{FALSE} because if something
fails a non meaningful error is going to be thrown. \code{TRUE} by default.}

\item{seed}{(\code{numeric(1)}) A value to be used as internal seed for
reproducible results. \code{NULL} by default.}

\item{verbose}{(\code{logical(1)}) Should the progress information be printed?
\code{TRUE} by default.}
}
\value{
An object of class \code{"RandomForestModel"} that inherits from classes
\code{"Model"} and \code{"R6"} with the fields:
\itemize{
\item \code{fitted_model}: An object of class \code{\link[randomForestSRC:rfsrc]{randomForestSRC::rfsrc()}} with the model.
\item \code{x}: The final \verb{\code{matrix}} used to fit the model.
\item \code{y}: The final \code{vector} or \code{data.frame} used to fit the model.
\item \code{hyperparams}: A \code{list} with all the provided hyperparameters.
\item \code{hyperparams_grid}: A \code{data.frame} with all the computed combinations of
hyperparameters and with one more column called \code{"loss"} with the value of
the loss function for each combination. The grid is ordered ascendingly by
loss value (the lower the better).
\item \code{best_hyperparams}: A \code{list} with the combination of hyperparameter with
the lowest loss value (the first row in \code{hyperparams_grid}).
\item \code{execution_time}: A \code{difftime} object with the total time taken to tune and
fit the model.
\item \code{removed_rows}: A \code{numeric} vector with the records' indices (in the
provided position) that were deleted and not taken in account in tunning
nor training.
\item \code{removed_x_cols}: A \code{numeric} vector with the columns' indices (in the
provided position and after the design matrix creation) that were deleted
and not taken in account in tunning nor training.
\item \code{...}: Some other parameters for internal use.
}
}
\description{
\code{random_forest()} is a wrapper of the \code{\link[randomForestSRC:rfsrc]{randomForestSRC::rfsrc()}} function
with the ability to tune the hyperparameters (grid search) in a simple way.
It fits univariate and multivariate models for numeric and/or categorical
response variables.

All the parameters marked as (\strong{tunable}) accept a vector of values with
wich the grid is generated for tuning. The returned object contains a
\code{data.frame} with the hyperparameter grid. In the end the best combination of
hyperparameters is used to fit the final model, which is also returned and
can be used to make new predictions.
}
\details{
\subsection{Tuning}{

The hyperparameters grid to evaluate in tuning is generated with the
cartesian product of all the provided values (all the posible combinations)
in all \strong{tunable} parameters. If only one value of each \strong{tunable}
parameter is provided no tuning is done. \code{tune_grid_proportion} allows you to
specify the proportion of all combinations you want to sample and tune, by
default all combinations (1) are evaluated.

The tuning algorithm works as follows:

\figure{tuning_algorithm.png}{Tuning algorithm}
}

Note that in univariate models with a numeric response variable \code{\link[=mse]{mse()}} (Mean
Squared Error) is used as loss function. In univariate models with a
categorical response variable (either binary or with more than two
categories) \code{\link[=pcic]{pcic()}} (Proportion of Cases Incorrectly Classified) is used.

\subsection{split_rule}{
\itemize{
\item \code{"mse"}: Implements weighted mean-squared error splitting for regression
analysis.
\item \code{"gini"}: Implements Gini index splitting for classification analysis.
\item \code{"auc"}: AUC (area under the ROC curve) splitting for both two-class and
multiclass setttings. AUC splitting is appropriate for imbalanced data.
\item \code{"entropy"}: entropy splitting for classification analysis.
\item Multivariate analysis: When one or both regression and classification
responses are detected, a multivariate normalized composite split rule of
mean-squared error and Gini index splitting is invoked.
}
}
}
\examples{
\dontrun{
# Fit with all default parameters
model <- random_forest(iris[, -5], iris$Species)

# With tuning
model <- random_forest(
  iris[, -1],
  iris$Sepal.Length,
  trees_number = c(100, 200, 300),
  node_size = c(1, 2),
  node_depth = c(10, 15)
)

predictions <- predict(model, iris)
predictions$predicted

# See the whole grid
model$hyperparams_grid

# Multivariate analysis
model <- random_forest(
  x = iris[, -c(1, 5)],
  y = iris[, c(1, 5)],
  sampled_x_vars_number = c(0.25, 0.75)
)
}

}
\seealso{
\code{\link[=predict.Model]{predict.Model()}}, \code{\link[=coef.Model]{coef.Model()}}

Other models: 
\code{\link{bayesian_model}()},
\code{\link{deep_learning}()},
\code{\link{generalized_boosted_machine}()},
\code{\link{generalized_linear_model}()},
\code{\link{partial_least_squares}()},
\code{\link{support_vector_machine}()}
}
\concept{models}
